{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Locality Sensitive Hash (LSH)\n",
      "\n",
      "Here at URX, we use locality sensitive hashes as a tool to identify similar text entities. Let's start with a simple example. Two strings \"My beautiful dog, Woof\" and \"My beautiful dog, Woo\" are similar but not the exact same. We would like to detect such cases. Conventional string similarity methods require either:\n",
      "\n",
      "1. Exact string matches or\n",
      "2. Computationally intensive methods\n",
      "\n",
      "Let's look at an example of 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My beautiful dog, Woof\"\n",
      "s2 = \"My beautiful dog, Woo\"\n",
      "\n",
      "print s1 == s2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unsurprisingly, the two strings are not literal equivalents. String comparisons are blazingly fast in python, through a combination of [hashtables and interning](http://stackoverflow.com/questions/12648002/what-could-affect-python-string-comparison-performance-for-strings-over-64-chara) but useless for the non-literal problem.\n",
      "\n",
      "A common pairwise, string similarity method is to calculate the [Levenshtein Distance](http://en.wikipedia.org/wiki/Levenshtein_distance). This metric calculates the _edit distance_ between two strings, i.e., the number of character edits (insertion, deletion or mutation) to change one string into a second string. In the case of \"ca**t**\" and \"ca**r**\" the edit distance is 1. The distance between **s1** and **s2** above is also one. The Levenshtein distance is commonly used across bioinformatics (matching genomic sequences), NLP and intervew fizz buzz questions. A naive, recursive implementation yields a $O(2^N)$ algorithm where N is the length of the longer string. A better, dynamic programming $O(N*M)$ algorithm is presented below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def cost(c1, c2):\n",
      "    \"\"\"\n",
      "    returns the cost of mutating character c1\n",
      "    into character c2. Bioinformaticians love\n",
      "    to insert heuristics here.\n",
      "    \"\"\"\n",
      "    return 1\n",
      "\n",
      "def LevenstheinDistance(s1, s2):\n",
      "    \"\"\"\n",
      "    Returns the minimum edit distance to transform\n",
      "    string s1 into string s2\n",
      "    \"\"\"\n",
      "    S = np.zeros((len(s1),len(s2)))\n",
      "    S[:,0] = range(len(s1))\n",
      "    S[0,:] = range(len(s2))\n",
      "    \n",
      "    for i in range(1, len(s1)):\n",
      "        for j in range(1, len(s2)):\n",
      "            if s1[i] == s2[j]:\n",
      "                S[i,j] = S[i-1,j-1]\n",
      "            else:\n",
      "                S[i,j] = min( [S[i-1,j] + 1,\n",
      "                               S[i,j-1] + 1,\n",
      "                               S[i-1,j-1] + cost(s1[i], s2[j])]\n",
      "                             )\n",
      "    return S[-1,-1]\n",
      "\n",
      "print LevenstheinDistance(s1, s2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An more memory efficent solution exists wherein only two rows of S are stored. Nevertheless, a polynomial (with respect to string length) solution requiring all-pairwise document comparisions is not tractible. We need a **fast** (i.e., sub-linear) method of quickly identifying nearly identical documents in a document store given a query document. For this reason, we turn to locality sensitive hashing. Let's take a look at how it works.\n",
      "\n",
      "Take one of our strings above \n",
      "\n",
      "--> \"My beautiful dog, Woof\".\n",
      "\n",
      "First, let's **clean** the string: remove punctuation, capitals and spaces \n",
      "\n",
      "--> \"mybeautifuldogwoof\"\n",
      "\n",
      "Now it's time for **feature extraction**. There are many means of extracting features from text, but perhaps the simplest is a sliding window, n-mer approach. Let's take a 3-mer sliding window\n",
      "\n",
      "--> S = [\"myb\", \"ybe\", \"bea\", \"eau\", ..., \"oof\"]\n",
      "\n",
      "Now begins the **hashing** component of LSH. Let's hash each feature into a vector, H:\n",
      "\n",
      "$$\n",
      "H = \\begin{pmatrix}h_1\\\\h_2\\\\...\\\\h_n\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "where $h_i$ is the 64-bit hash (md5, cityhash, etc) of n-mer $i$ in S. Each $h_i$ is _and-ed_ with a set of 64-bit binary masks, $m_j$, where \n",
      "\n",
      "$$\n",
      " m_j = \\begin{cases}\n",
      "        1  & bit\\  j\\\\\n",
      "        0 & else\n",
      "        \\end{cases}\n",
      "$$\n",
      "\n",
      "From this, a feature-hash is calculated using the following mapping\n",
      "\n",
      "$$\n",
      "V_i = SIGN\\left(\\sum_{j = 1}^{64}{h_i \\& m_j}\\right)\n",
      "$$\n",
      "\n",
      "where\n",
      "----\n",
      "\n",
      "$$\n",
      " h_i \\& m_j = \\begin{cases}\n",
      "        1  & if\\  1\\\\\n",
      "        -1 & if\\ 0\n",
      "        \\end{cases}\n",
      "$$\n",
      "\n",
      "and\n",
      "---\n",
      "\n",
      "$$\n",
      "SIGN\\left(x\\right) = \\begin{cases}\n",
      "                    1  & if\\  x>0\\\\\n",
      "                    0 & if\\ x\\leq0\n",
      "                    \\end{cases}\n",
      "$$\n",
      "\n",
      "Every n-mer in S contributes to each of 64 feature-hashses in $V_i$ via the above sum. Small changes in S will **not** result in large bit changes to $V_i$, rather this similarity hash is robust to small changes in S.\n",
      "\n",
      "This is the exact property we're looking for! A hash function which maps entities with small changes to the same (or nearly the same) bit vector. Let's try it out in practice"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Taken from:\n",
      "https://github.com/liangsun/simhash/blob/master/simhash/__init__.py\n",
      "\n",
      "Renamed Simhash to Bithash to avoid clash with Simhash-py and Simhash-db\n",
      "\"\"\"\n",
      "\n",
      "#Created by Liang Sun in 2013\n",
      "import re\n",
      "import logging\n",
      "import collections\n",
      "\n",
      "class Bithash(object):\n",
      "    def __init__(self, value, f=64, reg=ur'[\\w\\u4e00-\\u9fff]+', hashfunc=None):\n",
      "        '''\n",
      "        `f` is the dimensions of fingerprints\n",
      "\n",
      "        `reg` is meaningful only when `value` is basestring and describes\n",
      "        what is considered to be a letter inside parsed string. Regexp\n",
      "        object can also be specified (some attempt to handle any letters\n",
      "        is to specify reg=re.compile(r'\\w', re.UNICODE))\n",
      "\n",
      "        `hashfunc` accepts a utf-8 encoded string and returns a unsigned\n",
      "        integer in at least `f` bits.\n",
      "        '''\n",
      "\n",
      "        self.f = f\n",
      "        self.reg = reg\n",
      "        self.value = None\n",
      "\n",
      "        if hashfunc is None:\n",
      "            import hashlib\n",
      "            self.hashfunc = lambda x: int(hashlib.md5(x).hexdigest(), 16)\n",
      "        else:\n",
      "            self.hashfunc = hashfunc\n",
      "\n",
      "        if isinstance(value, Bithash):\n",
      "            self.value = value.value\n",
      "        elif isinstance(value, basestring):\n",
      "            self.build_by_text(unicode(value))\n",
      "        elif isinstance(value, collections.Iterable):\n",
      "            self.build_by_features(value)\n",
      "        elif isinstance(value, long):\n",
      "            self.value = value\n",
      "        else:\n",
      "            raise Exception('Bad parameter')\n",
      "\n",
      "    def _slide(self, content, width=3):\n",
      "        return [content[i:i+width] for i in xrange(max(len(content)-width+1, 1))]\n",
      "\n",
      "    def _tokenize(self, content):\n",
      "        ans = []\n",
      "        content = content.lower()\n",
      "        content = ''.join(re.findall(self.reg, content))\n",
      "        ans = self._slide(content)\n",
      "        return ans\n",
      "\n",
      "    def build_by_text(self, content):\n",
      "        features = self._tokenize(content)\n",
      "        self._features = features\n",
      "        return self.build_by_features(features)\n",
      "\n",
      "    def build_by_features(self, features):\n",
      "        hashs = [self.hashfunc(w.encode('utf-8')) for w in features]\n",
      "        v = [0]*self.f\n",
      "        masks = [1 << i for i in xrange(self.f)]\n",
      "        for h in hashs:\n",
      "            for i in xrange(self.f):\n",
      "                v[i] += 1 if h & masks[i] else -1\n",
      "        ans = 0\n",
      "        for i in xrange(self.f):\n",
      "            if v[i] >= 0:\n",
      "                ans |= masks[i]\n",
      "        self.value = ans\n",
      "\n",
      "    def distance(self, another):\n",
      "        x = (self.value ^ another.value) & ((1 << self.f) - 1)\n",
      "        ans = 0\n",
      "        while x:\n",
      "            ans += 1\n",
      "            x &= x-1\n",
      "        return ans\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My beautiful dog, Woof\"\n",
      "s2 = \"My beautiful dog, Woo\"\n",
      "\n",
      "b1 = Bithash(s1)\n",
      "b2 = Bithash(s2)\n",
      "\n",
      "print 'The bithash distance between s1 and s2 is %s' % b1.distance(b2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The bithash distance between s1 and s2 is 11\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Woah! There are 11 feature-hash bits (of 64) difference between S1 and S2! This is huge compared with the Levensthein distance calcualted previously. This large difference is due to the relatively small sizes of s1 and s2. Let's try a longer string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My absolutely gorgeous, wonderful, amazing, splendifioric, cute, intelligent, furry, faithful, beautiful dog, Woof\"\n",
      "s2 = \"My absolutely gorgeous, wonderful, amazing, splendifioric, cute, intelligent, furry, faithful, beautiful dog, Woo\"\n",
      "\n",
      "b1 = Bithash(s1)\n",
      "b2 = Bithash(s2)\n",
      "\n",
      "print 'The bithash distance between s1 and s2 is %s' % b1.distance(b2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The bithash distance between s1 and s2 is 2\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Locality Sensitive Hash (LSH)\n",
      "\n",
      "Here at URX, we use locality sensitive hashes as a tool to identify similar text entities. Let's start with a simple example. Two strings \"My beautiful dog, Woof\" and \"My beautiful dog, Woo\" are similar but not the exact same. We would like to detect such cases. Conventional string similarity methods require either:\n",
      "\n",
      "1. Exact string matches or\n",
      "2. Computationally intensive methods\n",
      "\n",
      "Let's look at an example of 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My beautiful dog, Woof\"\n",
      "s2 = \"My beautiful dog, Woo\"\n",
      "\n",
      "s1 == s2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unsurprisingly, the two strings are not literal equivalents. String comparisons are blazingly fast in python, through a combination of [hashtables and interning](http://stackoverflow.com/questions/12648002/what-could-affect-python-string-comparison-performance-for-strings-over-64-chara) but useless for the non-literal problem.\n",
      "\n",
      "A common pairwise, string similarity method is to calculate the [Levenshtein Distance](http://en.wikipedia.org/wiki/Levenshtein_distance). This metric calculates the _edit distance_ between two strings, i.e., the number of character edits (insertion, deletion or mutation) to change one string into a second string. \n",
      "\n",
      "For example, in the case of \"ca**t**\" and \"ca**r**\" the edit distance is 1. The distance between **s1** and **s2** above is also one. \n",
      "\n",
      "Now a naive implementation yields a $O(2^N)$ algorithm (this means it will take longer than we want), there are better $O(N*M)$ and $\\Theta(n^{1-\\Theta(1/log\\ log\\ n)})$ algorithms, but they are still too expensive to use in general."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An more memory efficent solution exists wherein only two rows of S are stored. Nevertheless, solution requiring all-pairwise document comparisions is not tractible. \n",
      "\n",
      "We need a **fast** method of quickly identifying nearly identical documents in a document store given a query document. \n",
      "\n",
      "For this reason, we turn to locality sensitive hashing. Let's take a look at how it works.\n",
      "\n",
      "Take one of our strings above "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's **clean** the string: remove punctuation, capitals and spaces "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean1 = s1.lower().replace(' ','').replace(',', '')\n",
      "clean1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'mybeautifuldogwoof'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean2 = s2.lower().replace(' ','').replace(',', '')\n",
      "clean2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "'mybeautifuldogwoo'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it's time for **feature extraction**. There are many means of extracting features from text, but perhaps the simplest is a sliding window, n-mer approach. Let's take a 3-mer sliding window"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def slide(content, width=3):\n",
      "    return [content[i:i+width] for i in range(max(len(content)-width+1, 1))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vector1 = slide(clean1)\n",
      "vector1[:5] # Only showing the first 5, so you get the basic idea"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "['myb', 'ybe', 'bea', 'eau', 'aut']"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vector2 = slide(clean2)\n",
      "vector2[:5] # Only showing the first 5, so you get the basic idea"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "['myb', 'ybe', 'bea', 'eau', 'aut']"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now begins the **hashing** component of LSH. \n",
      "\n",
      "With the above vector2, lets assume that we have extracted a number of *features* from the given text, the exact number of features only matters in relation to the overall accuracy needed from the hash.\n",
      "\n",
      "Now lets assume that we generate a range of hash tables, each representing a *band* or *bucket* where we can place bits we find when we derive a hash from each feature, again the number of buckets is not super important, it is a tradeoff between the speed at which we can produce the LSH and the accuracy we desire.\n",
      "\n",
      "This might seem really odd, but if you think about this, without focusing on the hash part it makes a lot of sense. Lets take our two feature vectors that we extracted from our two above strings. Measuring such differences is very typical, and there are many ways to do tbhis mathematically.\n",
      "\n",
      "Prehaps the most famous (but by no means the *only* option) is the [Jaccard index](http://en.wikipedia.org/wiki/Jaccard_index), mathematically this is given as \n",
      "\n",
      "$$JaccardIndex(A, B) = \\frac{\\left | A \\cap   B\\right | }{\\left | A \\cup  B\\right | } $$\n",
      "\n",
      "That might seem a lot to take in, but really all it says is that the index (or how similar) the two sets are is the size of everything that $A$ and $B$ share divided by the total amount of things in *both* sets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(vector1) & set(vector2)) / len(set(vector1) | set(vector2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.9375"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ".... 0.9375 ! These two sets are *very* similar, to give the counter example lets show two sets that are not similar"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(\"1234\") & set(\"4567\")) / len(set(\"1234\") | set(\"4567\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.14285714285714285"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "... As you can see, these two sets are not that similar (they only have the number 4 in common)\n",
      "\n",
      "So why do we use a hash instead of doing the direct set difference ? Well the basic reason is in the idea of reducing the problem to a simpler, more tractible one. Remember earlier when we said that pairwise comparison was expensive ? In using a hash we are able to **probabilistically** encode a solution to this problem, without having to directly tackle it.\n",
      "\n",
      "The neat thing here is that, in each *band* the probability that two similar hashes collide is higher, if the input features are similar. That is if we have two strings where some of the tokens hash to the same value, then *na\u00efvly* we expect that we will count more collisions than we would expect to observe from random noise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "If we work through this somewhat we can start to apply probability theory to begin to identify what is the likelyhood of having a collision vs random.\n",
      "\n",
      "This, in a visual form looks something like so\n",
      "\n",
      "![http://micvog.files.wordpress.com/2013/08/lsh1.png?w=682&h=420](http://micvog.files.wordpress.com/2013/08/lsh1.png?w=682&h=420)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Putting it together as a SimHash"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above construction is a simple Locality Sensitive Hash, using this, and the idea that our measure of similarity is some form of basic set difference we can put this togther with a well known implementation, in this case a simhash.\n",
      "\n",
      "Formally we define this like so:\n",
      "We hash each feature into a vector, $H$:\n",
      "\n",
      "$$\n",
      "H = \\begin{pmatrix}h_1\\\\h_2\\\\...\\\\h_n\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "... where $h_i$ is a 64-bit hash (jenkins, murmur3, cityhash, it does not matter) of n-mer $i$ in S. Each $h_i$ is _bitwise and_ of 64-bit binary masks, $m_j$, where \n",
      "\n",
      "$$\n",
      " m_j = \\begin{cases}\n",
      "        1  & bit\\  j\\\\\n",
      "        0 & else\n",
      "        \\end{cases}\n",
      "$$\n",
      "\n",
      "... from this, a feature-hash is calculated using the following mapping\n",
      "\n",
      "$$\n",
      "V_i = SIGN\\left(\\sum_{j = 1}^{64}{h_i \\& m_j}\\right)\n",
      "$$\n",
      "\n",
      "where, to calculate $h_i \\& m_j$ we make it produce a value of either 1 or -1\n",
      "\n",
      "$$\n",
      " h_i \\& m_j = \\begin{cases}\n",
      "        1  & if\\  1\\\\\n",
      "        -1 & if\\ 0\n",
      "        \\end{cases}\n",
      "$$\n",
      "\n",
      "and where we calculate the $SIGN$ as follows \n",
      "\n",
      "$$\n",
      "SIGN\\left(x\\right) = \\begin{cases}\n",
      "                    1  & if\\  x>0\\\\\n",
      "                    0 & if\\ x\\leq0\n",
      "                    \\end{cases}\n",
      "$$\n",
      "\n",
      "Every n-mer in S contributes to each of 64 feature-hashses in $V_i$ via the above sum. Small changes in S will **not** result in large bit changes to $V_i$, rather this similarity hash is robust to small changes in S.\n",
      "\n",
      "The formal version presented here can take some digestion, but remember it is essentially the basic idea discussed earlier when we outlined the basic idea of what consititutes a *Locality Sensitive Hash*.\n",
      "\n",
      "To help work through this, the formal procerss is also easy to show visually\n",
      "\n",
      "![http://static.oschina.net/uploads/img/201308/30125158_L1CI.jpg](http://static.oschina.net/uploads/img/201308/30125158_L1CI.jpg)\n",
      "\n",
      "This is the exact property we're looking for! A hash function which maps entities with small changes to the same (or nearly the same) bit vector. Let's try it out in practice"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Taken from:\n",
      "https://github.com/liangsun/simhash/blob/master/simhash/__init__.py\n",
      "\n",
      "Renamed Simhash to Bithash to avoid clash with Simhash-py and Simhash-db\n",
      "\"\"\"\n",
      "\n",
      "#Created by Liang Sun in 2013\n",
      "import re\n",
      "import logging\n",
      "import collections\n",
      "\n",
      "class Bithash(object):\n",
      "    def __init__(self, value, f=64, reg=ur'[\\w\\u4e00-\\u9fff]+', hashfunc=None):\n",
      "        '''\n",
      "        `f` is the dimensions of fingerprints\n",
      "\n",
      "        `reg` is meaningful only when `value` is basestring and describes\n",
      "        what is considered to be a letter inside parsed string. Regexp\n",
      "        object can also be specified (some attempt to handle any letters\n",
      "        is to specify reg=re.compile(r'\\w', re.UNICODE))\n",
      "\n",
      "        `hashfunc` accepts a utf-8 encoded string and returns a unsigned\n",
      "        integer in at least `f` bits.\n",
      "        '''\n",
      "\n",
      "        self.f = f\n",
      "        self.reg = reg\n",
      "        self.value = None\n",
      "\n",
      "        if hashfunc is None:\n",
      "            import hashlib\n",
      "            self.hashfunc = lambda x: int(hashlib.md5(x).hexdigest(), 16)\n",
      "        else:\n",
      "            self.hashfunc = hashfunc\n",
      "\n",
      "        if isinstance(value, Bithash):\n",
      "            self.value = value.value\n",
      "        elif isinstance(value, basestring):\n",
      "            self.build_by_text(unicode(value))\n",
      "        elif isinstance(value, collections.Iterable):\n",
      "            self.build_by_features(value)\n",
      "        elif isinstance(value, long):\n",
      "            self.value = value\n",
      "        else:\n",
      "            raise Exception('Bad parameter')\n",
      "\n",
      "    def _slide(self, content, width=3):\n",
      "        return [content[i:i+width] for i in xrange(max(len(content)-width+1, 1))]\n",
      "\n",
      "    def _tokenize(self, content):\n",
      "        ans = []\n",
      "        content = content.lower()\n",
      "        content = ''.join(re.findall(self.reg, content))\n",
      "        ans = self._slide(content)\n",
      "        return ans\n",
      "\n",
      "    def build_by_text(self, content):\n",
      "        features = self._tokenize(content)\n",
      "        self._features = features\n",
      "        return self.build_by_features(features)\n",
      "\n",
      "    def build_by_features(self, features):\n",
      "        hashs = [self.hashfunc(w.encode('utf-8')) for w in features]\n",
      "        v = [0]*self.f\n",
      "        masks = [1 << i for i in xrange(self.f)]\n",
      "        for h in hashs:\n",
      "            for i in xrange(self.f):\n",
      "                v[i] += 1 if h & masks[i] else -1\n",
      "        ans = 0\n",
      "        for i in xrange(self.f):\n",
      "            if v[i] >= 0:\n",
      "                ans |= masks[i]\n",
      "        self.value = ans\n",
      "\n",
      "    def distance(self, another):\n",
      "        x = (self.value ^ another.value) & ((1 << self.f) - 1)\n",
      "        ans = 0\n",
      "        while x:\n",
      "            ans += 1\n",
      "            x &= x-1\n",
      "        return ans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My beautiful dog, Woof\"\n",
      "s2 = \"My beautiful dog, Woo\"\n",
      "\n",
      "b1 = Bithash(s1)\n",
      "b2 = Bithash(s2)\n",
      "\n",
      "print 'The bithash distance between s1 and s2 is %s' % b1.distance(b2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The bithash distance between s1 and s2 is 11\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Woah! There are 11 feature-hash bits (of 64) difference between S1 and S2! This is huge compared with the Levensthein distance calcualted previously. This large difference is due to the relatively small sizes of s1 and s2. Let's try a longer string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My absolutely gorgeous, wonderful, amazing, splendifioric, cute, intelligent, furry, faithful, beautiful dog, Woof\"\n",
      "s2 = \"My absolutely gorgeous, wonderful, amazing, splendifioric, cute, intelligent, furry, faithful, beautiful dog, Woo\"\n",
      "\n",
      "b1 = Bithash(s1)\n",
      "b2 = Bithash(s2)\n",
      "\n",
      "print 'The bithash distance between s1 and s2 is %s' % b1.distance(b2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The bithash distance between s1 and s2 is 2\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Why this matters?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a huge number of use cases for a Locality Sensitive Hash, they turn up in Machine Learning, Information Retrevial and Computer Vision often due to the relative ease and low computational cost.\n",
      "\n",
      "Whilst more simplistic hash constructions (such as the above simhash) can, and do, produce lower quality results than more advanced algorithms they are also able to support extremely large datasets.\n",
      "\n",
      "Think back to the start, we are using Locality Sensitive Hashes to make it possible to crawl the Internet, can you personally think of a dataset that is larger than that !\n",
      "\n",
      "Any questions ? Want to know about more insane constructions of such things, like *semantic hashing*, or AND-OR random-projection hashes ? - Come and ask either Greg or Joe !"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}

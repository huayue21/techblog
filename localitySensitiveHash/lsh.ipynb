{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Locality Sensitive Hash (LSH)\n",
      "\n",
      "Here at URX, we use locality sensitive hashes as a tool to identify similar text entities. Let's start with a simple example. Two strings \"My beautiful dog, Woof\" and \"My beautiful dog, Woo\" are similar but not the exact same. We would like to detect such cases. Conventional string similarity methods require either:\n",
      "\n",
      "1. Exact string matches or\n",
      "2. Computationally intensive methods\n",
      "\n",
      "Let's look at an example of 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = \"My beautiful dog, Woof\"\n",
      "s2 = \"My beautiful dog, Woo\"\n",
      "\n",
      "print s1 == s2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unsurprisingly, the two strings are not literal equivalents. String comparisons are blazingly fast in python, through a combination of [hashtables and interning](http://stackoverflow.com/questions/12648002/what-could-affect-python-string-comparison-performance-for-strings-over-64-chara) but useless for the non-literal problem.\n",
      "\n",
      "A common pairwise, string similarity method is to calculate the [Levenshtein Distance](http://en.wikipedia.org/wiki/Levenshtein_distance). This metric calculates the _edit distance_ between two strings, i.e., the number of character edits (insertion, deletion or mutation) to change one string into a second string. In the case of \"ca**t**\" and \"ca**r**\" the edit distance is 1. The distance between **s1** and **s2** above is also one. The Levenshtein distance is commonly used across bioinformatics (matching genomic sequences), NLP and intervew fizz buzz questions. A naive, recursive implementation yields a $O(2^N)$ algorithm where N is the length of the longer string. A better, dynamic programming $O(N*M)$ algorithm is presented below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def cost(c1, c2):\n",
      "    \"\"\"\n",
      "    returns the cost of mutating character c1\n",
      "    into character c2. Bioinformaticians love\n",
      "    to insert heuristics here.\n",
      "    \"\"\"\n",
      "    return 1\n",
      "\n",
      "def LevenstheinDistance(s1, s2):\n",
      "    \"\"\"\n",
      "    Returns the minimum edit distance to transform\n",
      "    string s1 into string s2\n",
      "    \"\"\"\n",
      "    S = np.zeros((len(s1),len(s2)))\n",
      "    S[:,0] = range(len(s1))\n",
      "    S[0,:] = range(len(s2))\n",
      "    \n",
      "    for i in range(1, len(s1)):\n",
      "        for j in range(1, len(s2)):\n",
      "            if s1[i] == s2[j]:\n",
      "                S[i,j] = S[i-1,j-1]\n",
      "            else:\n",
      "                S[i,j] = min( [S[i-1,j] + 1,\n",
      "                               S[i,j-1] + 1,\n",
      "                               S[i-1,j-1] + cost(s1[i], s2[j])]\n",
      "                             )\n",
      "    return S[-1,-1]\n",
      "\n",
      "print LevenstheinDistance(s1, s2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An more memory efficent solution exists wherein only two rows of S are stored. Nevertheless, a polynomial (with respect to string length) solution requiring all-pairwise document comparisions is not tractible. We need a **fast** (i.e., sub-linear) method of quickly identifying nearly identical documents in a document store given a query document. For this reason, we turn to locality sensitive hashing. Let's take a look at how it works.\n",
      "\n",
      "Take one of our strings above \n",
      "\n",
      "--> \"My beautiful dog, Woof\".\n",
      "\n",
      "First, let's **clean** the string: remove punctuation, capitals and spaces \n",
      "\n",
      "--> \"mybeautifuldogwoof\"\n",
      "\n",
      "Now it's time for **feature extraction**. There are many means of extracting features from text, but perhaps the simplest is a sliding window, n-mer approach. Let's take a 3-mer sliding window\n",
      "\n",
      "--> S = [\"myb\", \"ybe\", \"bea\", \"eau\", ..., \"oof\"]\n",
      "\n",
      "Now begins the **hashing** component of LSH. Let's hash each feature into a vector, H:\n",
      "\n",
      "$$\n",
      "H = \\begin{pmatrix}h_1\\\\h_2\\\\...\\\\h_n\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "where $h_i$ is the 64-bit hash (md5, cityhash, etc) of n-mer $i$ in S. Each $h_i$ is _and-ed_ with a set of 64-bit binary masks, $m_j$, where \n",
      "\n",
      "$$\n",
      " m_j = \\begin{cases}\n",
      "        1  & bit\\  j\\\\\n",
      "        0 & else\n",
      "        \\end{cases}\n",
      "$$\n",
      "\n",
      "From this, a feature-hash is calculated using the following mapping\n",
      "\n",
      "$$\n",
      "V_i = SIGN\\left(\\sum_{j = 1}^{64}{h_i \\& m_j}\\right)\n",
      "$$\n",
      "\n",
      "where\n",
      "----\n",
      "\n",
      "$$\n",
      " h_i \\& m_j = \\begin{cases}\n",
      "        1  & if\\  1\\\\\n",
      "        -1 & if\\ 0\n",
      "        \\end{cases}\n",
      "$$\n",
      "\n",
      "and\n",
      "---\n",
      "\n",
      "$$\n",
      "SIGN\\left(x\\right) = \\begin{cases}\n",
      "                    1  & if\\  x>0\\\\\n",
      "                    0 & if\\ x\\leq0\n",
      "                    \\end{cases}\n",
      "$$\n",
      "\n",
      "Every n-mer in S contributes to each of 64 feature-hashses in $V_i$ via the above sum. Small changes in S will **not** result in large bit changes to $V_i$, rather this similarity hash is robust to small changes in S.\n",
      "\n",
      "This is the exact property we're looking for! A hash function which maps entities with small changes to the same (or nearly the same) bit vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
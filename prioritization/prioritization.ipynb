{
 "metadata": {
  "name": "",
  "signature": "sha256:47c099e1149881555a33c4a4f634cf44b4fae53cfaa5692798f0ab46b50ab07e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction\n",
      "\n",
      "Here at [URX](http://urx.com/?utm_source=blog&utm_medium=post&utm_campaign=web-crawl-3) we've built the [world's first API](https://developers.urx.com/) for developers to monetize off of context relevant actions. To ensure the content we surface on publisher sites and applications is relevant, we back this API with a search engine. The engine contains a large corpus of web documents, meticulously maintained and carefully grown by crawling internet content. We've come to discover that building a functional crawler can be done relatively cheaply, but building a robust crawler requires overcoming a few technical challenges. \n",
      "\n",
      "In [Part 1](http://blog.urx.com/urx-blog/2014/9/4/the-science-of-crawl-part-1-deduplication-of-web-content), we introduced a funnel for deduplicating web documents within a search index. The dual problems of exact duplicate and near duplicate web document identification are considered. By chaining together several methods with increasing specificity we identify a system which provides sufficient precision and recall with minimal computational trade-offs.\n",
      "\n",
      "In this post, we look at the challenge of __prioritizing__ which web documents to capture first. To ensure our search engine always contains relevant results, we need a crawler which continuously discovers new content. To provide new content into the engine, a persistent web crawler extracts previously unseen links within pages, and adds the links onto a priority queue. When a new link is popped off the queue, the link is followed, the resulting content is downloaded and indexed into the search engine. \n",
      "\n",
      "Naively, the queue could be ordered by time since the link was discovered. This is a classic FIFO priority queue. However, given the scale at which URX crawls web content, we have hundreds of millions of uncrawled links queueing at any given time. With so many links to follow it becomes difficult to balance the finite resources devoted to crawling new pages. See [part 2](http://blog.urx.com/urx-blog/2014/10/23/the-science-of-crawl-part-2-content-freshness) for a brief description of crawler resource allocation.\n",
      "\n",
      "With so many uncrawled links, it is necessary to think of how to prioritize this queue. Research within the field of prioritization can be broken into two areas: __query relevance__ and __query-independent importance__. The former refers to biasing a crawler to download content most relevant to the content actively searched. The latter refers to leveraging the graph structure of web links to encourage an \"importance\" based prioritization. There are tradeoffs to both. It's difficult to evaluate the correlation between unseen links and search intents. The only information present to use is the link itself and any surround anchor text. This creates massively sparse and potentially inaccurate relevancy estimates. Similarly, while query independent methods seek \"important\" pages first, common importance metrics do not necessarily tie to improved user experience.\n",
      "\n",
      "# Query-independent importance\n",
      "\n",
      "The first facet of prioritization estimates page importance independent of user search queries. Larry Page's famous [page rank](http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf) is perhaps the most popular method for numerically estimating the importance of pages. The method attempts to solve the iterative equation (taken from [Wikipedia](http://en.wikipedia.org/wiki/PageRank))\n",
      "\n",
      "$$\n",
      "PR(u) = \\sum_{v\\in B_u}{\\frac{PR(v)}{L(v)}}\n",
      "$$\n",
      "\n",
      "where $PR(u)$ is the relative rank of a page $u$, $B_u$ is the set of all pages pointing to $u$, and $L(v)$ is the number of out-links in page $v$. \n",
      "\n",
      "At a high level, a page is important if 1. many pages point into that page and 2. the in-pointing pages are themselves, important. Page rank problem is well studied. It was discovered that solutions of this iterative system are degenerate - often converging to the page with highest rank. To fix this issue, it was noted that adding a small connection weight between all pairs of pages created a more stable system. This can be realized by adding a damping factor $d$ normalized by the number of pages $N$ to the above equation:\n",
      "\n",
      "$$\n",
      "PR(u) = \\frac{1-d}{N} + d\\sum_{v\\in B_u}{\\frac{PR(v)}{L(v)}}\n",
      "$$\n",
      "\n",
      "Unfortunately, by adding a damping factor to every connection, the page rank adjacency matrix becomes a prohibitively dense, large matrix. It quickly becomes impossible to store in memory.\n",
      "\n",
      "Below is a simple page rank implementation taken from one of [Apache Spark's examples](https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py) with a damping factor of 0.85\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Taken from https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py\n",
      "#\n",
      "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "# contributor license agreements.  See the NOTICE file distributed with\n",
      "# this work for additional information regarding copyright ownership.\n",
      "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "# (the \"License\"); you may not use this file except in compliance with\n",
      "# the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "\n",
      "\"\"\"\n",
      "This is an example implementation of PageRank. For more conventional use,\n",
      "Please refer to PageRank implementation provided by graphx\n",
      "\"\"\"\n",
      "\n",
      "import re\n",
      "import sys\n",
      "from operator import add\n",
      "\n",
      "from pyspark import SparkContext\n",
      "\n",
      "\n",
      "def computeContribs(urls, rank):\n",
      "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
      "    num_urls = len(urls)\n",
      "    for url in urls:\n",
      "        yield (url, rank / num_urls)\n",
      "\n",
      "\n",
      "def parseNeighbors(urls):\n",
      "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
      "    parts = re.split(r'\\s+', urls)\n",
      "    return parts[0], parts[1]\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    if len(sys.argv) != 3:\n",
      "        print >> sys.stderr, \"Usage: pagerank <file> <iterations>\"\n",
      "        exit(-1)\n",
      "\n",
      "    print >> sys.stderr,  \"\"\"WARN: This is a naive implementation of PageRank and is\n",
      "          given as an example! Please refer to PageRank implementation provided by graphx\"\"\"\n",
      "\n",
      "    # Initialize the spark context.\n",
      "    sc = SparkContext(appName=\"PythonPageRank\")\n",
      "\n",
      "    # Loads in input file. It should be in format of:\n",
      "    #     URL         neighbor URL\n",
      "    #     URL         neighbor URL\n",
      "    #     URL         neighbor URL\n",
      "    #     ...\n",
      "    lines = sc.textFile(sys.argv[1], 1)\n",
      "\n",
      "    # Loads all URLs from input file and initialize their neighbors.\n",
      "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
      "\n",
      "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
      "    ranks = links.map(lambda (url, neighbors): (url, 1.0))\n",
      "\n",
      "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
      "    for iteration in xrange(int(sys.argv[2])):\n",
      "        # Calculates URL contributions to the rank of other URLs.\n",
      "        contribs = links.join(ranks).flatMap(\n",
      "            lambda (url, (urls, rank)): computeContribs(urls, rank))\n",
      "\n",
      "        # Re-calculates URL ranks based on neighbor contributions.\n",
      "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
      "\n",
      "    # Collects all URL ranks and dump them to console.\n",
      "    for (link, rank) in ranks.collect():\n",
      "        print \"%s has rank: %s.\" % (link, rank)\n",
      "\n",
      "    sc.stop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named pyspark",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-18-9c55015cb20b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mImportError\u001b[0m: No module named pyspark"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For those who enjoy scala, a much more efficient page rank example can be seen in one of [Apache GraphX examples](https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/PageRank.scala).\n",
      "\n",
      "Many have studied ways to improve the page rank calculation. One popular approach is the [Adaptive Online Page Importance Computation](http://www2003.org/cdrom/papers/refereed/p007/p7-abiteboul.html) which modifies the above system of equations to avoid holding the full Page Rank adjacency matrix in memory. AOIPIC has been shown to approximate the Page Rank score after a sufficient number of iterations. [Ricardo Baeza-Yates et. al](http://dl.acm.org/citation.cfm?id=1062768) provide a comparison of page rank with OPIC (non adaptive, APOIC) and simple back-link count. \n",
      "\n",
      "# Query relevant prioritiziation\n",
      "\n",
      "The goal of a query based prioritization scheme is to rank unseen links in the priority queue as a function of relevance to incoming search queries. One specific implementation is diagrammed below, taken from Figure 4 from [Olston's recent paper](http://www.cs.cmu.edu/~spandey/wsdm-pandey.pdf).\n",
      "<img src=\"queryflow.png\" />\n",
      "\n",
      "At a high level, Olston's goal is to identify \"needy\" queries - i.e., queries whose top K-results do not contain relevant results. The *type* of content needed to improve these queries is identified and used to prioritize uncrawled links likely to contain such content. Using this methodology, underperforming queries are continuously supplied with new content. possible.\n",
      "\n",
      "The first step is to build an index of \"needy\" queries, those queries which stand to benefit the most from new, diverse content. In the definition of Olston, a search query is needy if the average relevancy score returned in the top-K results is less than a predefined threshold. Defining search relevancy is a subtle topic deserving of its own blog post. I recommend [Buettcher's book](http://www.amazon.com/Information-Retrieval-Implementing-Evaluating-Engines/dp/0262026511/) on Information Retrieval for a full history and quantification of search relevancy. In short, relevancy is commonly calculated as a function of number of returned documents, [BM-25 scoring](http://en.wikipedia.org/wiki/Okapi_BM25) and click through rate.\n",
      "\n",
      "Given an index of needy queries, a function can be constructed to measure the priority of a given uncrawled page, $p$. Olston writes this function as:\n",
      "\n",
      "$$\n",
      "P(p, Q) = \\sum_{q\\in Q} f(q) * I(p, q)\n",
      "$$\n",
      "\n",
      "Where \n",
      "```\n",
      "Q is the index of needy queries\n",
      "f(q) is the frequency of a given query\n",
      "I(p, q) is an indicator of whether an unseen page, p, matches query, q\n",
      "```\n",
      "Pages matching needy, frequently searched queries will have the largest prioritization scores. Similarly pages which are either infrequently searched or containing rich results will be de-prioritized.\n",
      "\n",
      "To measure $I(p, q)$, Olston describes an interesting, approach which seeks to jointly maximize the expected priority across all unseen pages and queries. Unfortunately in the worst case, this full expectation maximization is NP hard. To reduce complexity, Olston suggests [w-shingling](http://en.wikipedia.org/wiki/W-shingling) pages and queries. If the w-shingles of the query and page match $\\rho$ fraction of shingles, $I(p, q)$ will be 1. Otherwise $I(p, q)$ will be 0. \n",
      "\n",
      "There exists a catch-22. If a page has not yet been crawled, its content is unknown. However, the decision to crawl a page is biased by the content of a page. To approximate a solution, Olston describes an uncrawled \"page\" as the tokens generated from a uncrawled URL along with any surrounding anchor text. For example, in [the following article](http://www.nytimes.com/2015/03/31/arts/television/trevor-noah-to-succeed-jon-stewart-on-the-daily-show.html) from The New York Times, we see a link:\n",
      "\n",
      "```\n",
      "<a href=\"http://thedailyshow.cc.com/news-team/samantha-bee\">Samantha Bee</a>\n",
      "```\n",
      "which can be tokenized into the following words \n",
      "\n",
      "```\n",
      "p = [news, team, samantha, bee]\n",
      "```\n",
      "\n",
      "We can write a custom tokenizer to split urls and anchor text into \"words\" by splitting on non alphanumeric words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_get_unicode(raw):\n",
      "    if not raw:\n",
      "        return ''\n",
      "    badchrs = set(['.', ',', ';', '\\'', \n",
      "                   '\\\\', '/', '!', '@', \n",
      "                   '#', '$', '%', '^', \n",
      "                   '&', '*', '(', ')', \n",
      "                   '~', '`', '-', '{', \n",
      "                   '}', '[', ']', ':', \n",
      "                   '<', '>', '?', '\\n', \n",
      "                   '\\t', '\\r', '\"'])\n",
      "    prevWasBad = False\n",
      "    prevWasSpace = False\n",
      "    ret = []\n",
      "    for c in raw:\n",
      "        if not c.isdigit() and not c in badchrs:\n",
      "            if c == ' ':\n",
      "                if prevWasSpace:\n",
      "                    continue\n",
      "                prevWasSpace = True\n",
      "            else:\n",
      "                prevWasSpace = False\n",
      "            prevWasBad = False\n",
      "            ret.append(c)\n",
      "        else:\n",
      "            if prevWasBad or prevWasSpace:\n",
      "                continue\n",
      "            else:\n",
      "                prevWasBad = True\n",
      "                ret.append(' ')\n",
      "                prevWasSpace = True\n",
      "\n",
      "    return ''.join(ret)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://thedailyshow.cc.com/news-team/samantha-bee'\n",
      "atext = 'Samantha Bee'\n",
      "\n",
      "url_words = clean_get_unicode(url.lower()).split(' ')\n",
      "atext_words = clean_get_unicode(atext.lower()).split(' ')\n",
      "page = list(set(url_words).union(set(atext_words)))\n",
      "print page"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['http', 'cc', 'samantha', 'team', 'bee', 'thedailyshow', 'news', 'com']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can clean the remaining tokens further by removing common html words (`http`, `www`, `com`, etc.) and filtering the remaining words against an English dictionary. Query tokenization is performed using the same code. A mapping from a query to the set of relevant pages for that query can then be constructed. Such mappings are referred to as *needy query sketches*. \n",
      "\n",
      "There are many ways to construct needy query sketches. One simple methodology is to index the set of needy queries and their frequencies using a tool like [Elasticsearch](http://www.elastic.co/guide/). Page tokens are then used as search query. The frequencies the returned needy queries can then be summed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Combining strategies\n",
      "\n",
      "A final priority weighting can be determined using a linear combination of query-independent and query-dependent relevance scores. The exact weighting of the two can be determined through classic machine learning for ensemble fusion. The objective function is problem dependent but likely is a function of overall search relevance and user click through rates.\n",
      "\n",
      "Questions on the content? Interested in a position at URX? Please reach out to research@urx.com\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}